// Databricks notebook source
// MAGIC %md # 2015 NYC Taxi Dashboard Pipeline Queries
// MAGIC * Use this notebook to create the prep notebook 
// MAGIC * as well as the SQL and Scala commands needed for the Delta Live Table demo
// MAGIC * Uses map_point2Location data generated by this [GeoMesa + H3 Notebook notebook]($./GeoMesa + H3 Notebook) from the blog [Processing Geospatial Data at Scale With Databricks](https://databricks.com/blog/2019/12/05/processing-geospatial-data-at-scale-with-databricks.html)

// COMMAND ----------

// MAGIC %md ## Tasks required PRIOR to running the **2015 NYC Taxi Dashboard Pipeline**
// MAGIC * Build the `map_point2Location data` generated by this [GeoMesa + H3 Notebook notebook]($./GeoMesa + H3 Notebook) 
// MAGIC * Follow the first three steps of *this notebook*
// MAGIC   * Build the origin data ala `nyctaxi_greencab_origin` 
// MAGIC   * We will use this origin table so we can *manually* push data into our *pre-bronze or source* table `nyctaxi_greencab_source` 
// MAGIC   * This source table is what the the pipeline will read 

// COMMAND ----------

// MAGIC %md ### 1. Build Origin Table 
// MAGIC * The follow commands build the `nyctaxi_greencab_origin` origin table 
// MAGIC * We will use this table to feed the `nyctaxi_greencab_source` source table that the pipeline will read

// COMMAND ----------

// MAGIC %md #### 1.1 Load NYC Taxi GreenCab data from `/databricks-datasets/`
// MAGIC * Generate `greencab_2015` DataFrame that will be used to create the `nyctaxi_greencab_origin` table

// COMMAND ----------

// Load GreenCab 2015 data
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types._

val schema_green = new StructType()
    .add("VendorID", IntegerType)
    .add("lpep_pickup_datetime", TimestampType)
    .add("lpep_dropoff_datetime", TimestampType)
    .add("store_and_fwd_flag", StringType)
    .add("RatecodeID", IntegerType)
    .add("Pickup_longitude", StringType)
    .add("Pickup_latitude", StringType)
    .add("Dropoff_longitude", StringType)
    .add("Dropoff_latitude", StringType)
    .add("passenger_count", IntegerType)
    .add("trip_distance", DoubleType)
    .add("fare_amount", DoubleType)
    .add("extra", DoubleType)
    .add("mta_tax", DoubleType)
    .add("tip_amount", DoubleType)
    .add("tolls_amount", DoubleType)
    .add("ehail_fee", StringType)
    .add("improvement_surcharge", DoubleType)
    .add("total_amount", DoubleType)
    .add("payment_type", IntegerType)
    .add("trip_type", IntegerType)

val df = spark.read.option("header", true).schema(schema_green).csv("/databricks-datasets/nyctaxi/tripdata/green/green_tripdata_2015*.csv.gz")
df.createOrReplaceTempView("greencab_2015")

// View Data Frame
display(df)

// COMMAND ----------

// MAGIC %md #### 1.2 Build Origin Table `DAIS21.nyctaxi_greencab_origin`
// MAGIC * Drop table if exists
// MAGIC * Remove the underlying folder 
// MAGIC * Create the origin table and optimize it

// COMMAND ----------

// MAGIC %sql
// MAGIC -- drop table
// MAGIC DROP TABLE IF EXISTS cchalc.nyctaxi_greencab_origin;

// COMMAND ----------

dbutils.fs.rm("/user/christopher.chalcraft@databricks.com/nyctaxi/nyctaxi_greencab_origin", recurse=true)

// COMMAND ----------

// MAGIC %sql
// MAGIC -- Create origin table 
// MAGIC CREATE TABLE cchalc.nyctaxi_greencab_origin
// MAGIC USING delta
// MAGIC LOCATION "/user/christopher.chalcraft@databricks.com/nyctaxi/nyctaxi_greencab_origin"
// MAGIC AS (
// MAGIC SELECT * FROM greencab_2015
// MAGIC );
// MAGIC 
// MAGIC -- Optimize table
// MAGIC OPTIMIZE cchalc.nyctaxi_greencab_origin;

// COMMAND ----------

// MAGIC %md ### 2. Generate Initial Table
// MAGIC We will create the initial table `nyctaxi_greencab_source` that the pipeline will use
// MAGIC * Drop table if exists
// MAGIC * Remove the underlying folder 
// MAGIC * Create the source table

// COMMAND ----------

// MAGIC %sql
// MAGIC -- drop table
// MAGIC DROP TABLE IF EXISTS cchalc.nyctaxi_greencab_source;

// COMMAND ----------

dbutils.fs.rm("/user/christopher.chalcraft@databricks.com/nyctaxi/nyctaxi_greencab_source", recurse=true)

// COMMAND ----------

// MAGIC %sql
// MAGIC -- Initially set of data
// MAGIC CREATE TABLE cchalc.nyctaxi_greencab_source
// MAGIC USING delta
// MAGIC LOCATION "/user/christopher.chalcraft@databricks.com/nyctaxi/nyctaxi_greencab_source"
// MAGIC AS (
// MAGIC SELECT *
// MAGIC   FROM cchalc.nyctaxi_greencab_origin
// MAGIC  WHERE DATE(lpep_dropoff_datetime) = "2015-03-01"
// MAGIC ) 

// COMMAND ----------

dbutils.notebook.exit("Stop the notebook")

// COMMAND ----------

// MAGIC %md ### You can now run the *2015 NYCTaxi Dashboard Pipeline* IF ...
// MAGIC ** you also generated the `map_point2Location` table per this [GeoMesa + H3 Notebook notebook]($./GeoMesa + H3 Notebook) notebook **

// COMMAND ----------

// MAGIC %md ### 3. Load Data
// MAGIC * Run this statement and to add more daily data 

// COMMAND ----------

// MAGIC %sql
// MAGIC -- Next day
// MAGIC INSERT INTO cchalc.nyctaxi_greencab_source
// MAGIC SELECT *
// MAGIC   FROM cchalc.nyctaxi_greencab_origin
// MAGIC -- WHERE DATE(lpep_dropoff_datetime) = "2015-03-02"
// MAGIC  WHERE DATE(lpep_dropoff_datetime) = "2015-03-03"
// MAGIC --  WHERE DATE(lpep_dropoff_datetime) = "2015-03-04"
// MAGIC --  WHERE DATE(lpep_dropoff_datetime) = "2015-03-05"
// MAGIC --  WHERE DATE(lpep_dropoff_datetime) = "2015-03-06"

// COMMAND ----------



// COMMAND ----------

// MAGIC %md ## Miscellaenous Queries
// MAGIC You do **NOT** need to run any of the queries below; these are sample queries that were used to create the pipeline

// COMMAND ----------

// MAGIC %md ### Mapping Data

// COMMAND ----------

// Define schemas
// val schema_taxi_lookup = new StructType()
//   .add("LocationID", IntegerType)
//   .add("Borough", StringType)
//   .add("Zone", StringType)
//   .add("service_zone", StringType)

val schema_taxi_rate_code = new StructType()
  .add("RateCodeID", IntegerType)
  .add("RateCodeDesc", StringType)

val schema_taxi_payment_type = new StructType()
  .add("payment_type", IntegerType)
  .add("payment_desc", StringType)

// COMMAND ----------

// Define mapping tables
// val map_Location = spark.read.format("csv")
//         .schema(schema_taxi_lookup)
//         .option("delimiter", ",")
//         .option("header", "true")
//         .load("/databricks-datasets/nyctaxi/taxizone/taxi_zone_lookup.csv")

// Taxi Rate Code
val map_rateCode = spark.read.format("csv")
      .schema(schema_taxi_rate_code)
      .option("delimiter", ",")
      .option("header", "true")
      .load("/databricks-datasets/nyctaxi/taxizone/taxi_rate_code.csv")

// Taxi Payment Type
val map_paymentType = spark.read.format("csv")
      .schema(schema_taxi_payment_type)
      .option("delimiter", ",")
      .option("header", "true")
      .load("/databricks-datasets/nyctaxi/taxizone/taxi_payment_type.csv")

// COMMAND ----------

display(map_rateCode)

// COMMAND ----------

display(map_paymentType)

// COMMAND ----------

val map_point2Location = spark.read.format("delta").load("/user/christopher.chalcraft@databricks.com/nyctaxi/map_point2Location")
display(map_point2Location)

// COMMAND ----------

// MAGIC %md ### Raw Table

// COMMAND ----------

// Green Cab 2015 View
//   createView("raw_GreenCab")
//     .query {
//         spark.read.format("delta").load("/user/denny.lee/nyctaxi/nyctaxi_greencab_2019") 
//     }
//     .expect("valid pickup_datetime", "lpep_pickup_datetime IS NOT NULL")   
//     .expect("valid dropoff_datetime", "lpep_dropoff_datetime IS NOT NULL")

val raw_GreenCab = spark.read.format("delta").load("/user/christopher.chalcraft@databricks.com/nyctaxi/nyctaxi_greencab_2015") 

// COMMAND ----------

// MAGIC %md ### Bronze Table

// COMMAND ----------

// // bronze_GreenCab
// createTable("bronze_GreenCab")
//   .query {
//       read("raw_GreenCab")
//       .withColumn("lpep_dropoff_date", expr("DATE(lpep_dropoff_datetime)"))
//   }
//  .partitionBy("lpep_dropoff_date")
//  .tableProperty("pipelines.autoOptimize.zOrderCols", "lpep_dropoff_datetime")
//  .tableProperty("pipelines.metastore.tableName", "DAIS21.bronze_GreenCab")
//  .expectOrFail("valid lpep_dropoff_date", "lpep_dropoff_date IS NOT NULL")

val bronze_GreenCab = raw_GreenCab.withColumn("lpep_dropoff_date", expr("DATE(lpep_dropoff_datetime)"))

// COMMAND ----------

display(bronze_GreenCab)

// COMMAND ----------

// MAGIC %md ### Silver Tables

// COMMAND ----------

// // Payment Information
// createTable("silver_paymentInfo")
//   .query {
//       val map_paymentType = read("map_paymentType")
//       val map_location = read("map_location")
//       val paymentInfo = read("bronze_GreenCab")
//       paymentInfo
//         .join(map_paymentType, (paymentInfo("payment_type") === map_paymentType("payment_type")))
//         .join(map_location, (paymentInfo("DOLocationID") === map_location("LocationID")))
//         .select("lpep_dropoff_date", "lpep_dropoff_datetime", "passenger_count",
//                 "payment_desc", "Borough", "Zone", "service_zone",
//                 "fare_amount", "extra", "tip_amount", "tolls_amount", "total_amount")
//         .withColumn("hour", expr("HOUR(lpep_dropoff_datetime) AS hour"))
//   }
//   .partitionBy("lpep_dropoff_date")
//   .tableProperty("pipelines.autoOptimize.zOrderCols", "lpep_dropoff_datetime")    
//   .expect("non zero passenger count", "passenger_count > 0")
//   //.expectOrFail("non zero passenger count", "passenger_count > 0")
//   //.expectOrDrop("non zero passenger count", "passenger_count > 0")
  // Payment Information

val bronze_GreenCabM = bronze_GreenCab
          .withColumn("hour", expr("HOUR(lpep_dropoff_datetime) AS hour"))
          .withColumnRenamed("lpep_dropoff_datetime", "do_datetime")
          .withColumnRenamed("lpep_pickup_datetime", "pu_datetime")
          .withColumnRenamed("lpep_dropoff_date", "do_date")
          .withColumnRenamed("dropoff_latitude", "do_lat")
          .withColumnRenamed("dropoff_longitude", "do_long")


val silver_GreenCab = bronze_GreenCabM
          .join(map_rateCode, (bronze_GreenCabM("RateCodeID") === map_rateCode("RateCodeID")))
          .join(map_paymentType, (bronze_GreenCabM("payment_type") === map_paymentType("payment_type")))
          .join(map_point2Location, 
                 (bronze_GreenCabM("do_lat") === map_point2Location("dropoff_latitude")) && 
                 (bronze_GreenCabM("do_long") === map_point2Location("dropoff_longitude")) &&
                 (bronze_GreenCabM("do_datetime") === map_point2Location("lpep_dropoff_datetime"))
               )
          .select("do_datetime", "pu_datetime", "do_date", "hour", "passenger_count", "do_lat", "do_long",
                  "RateCodeDesc", "payment_desc", "borough", "zone", 
                  "fare_amount", "extra", "tip_amount", "tolls_amount", "total_amount")

// View data
display(silver_GreenCab)

// COMMAND ----------

// MAGIC %md ### Gold Tables

// COMMAND ----------

// createTable("gold_summaryStats")  
//   .query {
//       val summaryStats = read("bronze_GreenCab")
//       summaryStats.groupBy("lpep_dropoff_date").agg(
//           expr("COUNT(DISTINCT lpep_pickup_datetime) AS pickups"), 
//           expr("COUNT(DISTINCT lpep_dropoff_datetime) AS dropoffs"),
//           expr("COUNT(1) AS trips")
//       )
//   }
//   .tableProperty("pipelines.metastore.tableName", "DAIS21.gold_summaryStats")

val gold_summaryStats = silver_GreenCab
    .groupBy("do_date").agg(
        expr("COUNT(DISTINCT pu_datetime) AS pickups"), 
        expr("COUNT(DISTINCT do_datetime) AS dropoffs"),
        expr("COUNT(1) AS trips")
      )

display(gold_summaryStats)

// COMMAND ----------

//   // Payment Type By Hour (Car sizes <= 5 )
//   createTable("gold_carPaymentByHour")
//     .query {
//         val paymentTypeByHour = read("silver_paymentInfo") 
//         paymentTypeByHour.select("lpep_dropoff_date", "hour", "payment_desc", "total_amount").where(expr("passenger_count <= 5"))
//     }
//     .tableProperty("pipelines.metastore.tableName", "DAIS21.gold_carPaymentByHour")

val gold_paymentByHour = silver_GreenCab.groupBy("hour", "payment_desc").agg(expr("SUM(total_amount) AS total_amount"))
display(gold_paymentByHour)

// COMMAND ----------

val gold_bronx = silver_GreenCab.where(expr("borough = 'Bronx'")).select("do_datetime", "do_lat", "do_long", "RateCodeDesc", "payment_desc", "zone", "total_amount")
display(gold_bronx)

// COMMAND ----------

val gold_brooklyn = silver_GreenCab.where(expr("borough = 'Brooklyn'")).select("do_datetime", "do_lat", "do_long", "RateCodeDesc", "payment_desc", "zone", "total_amount")
display(gold_brooklyn)

// COMMAND ----------

val gold_queens = silver_GreenCab.where(expr("borough = 'Queens'")).select("do_datetime", "do_lat", "do_long", "RateCodeDesc", "payment_desc", "zone", "total_amount")
display(gold_queens)

// COMMAND ----------

// MAGIC %md ### Review Event Logs for Expectations Data

// COMMAND ----------

// MAGIC %fs ls dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT details FROM delta.`dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/`
// MAGIC WHERE details LIKE '%flow_progress%data_quality%'

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT * FROM delta.`dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/`
// MAGIC WHERE details LIKE '%flow_progress%data_quality%expectations%'

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT id, origin.pipeline_id, origin.pipeline_name, origin.cluster_id, origin.flow_id, origin.flow_name, * FROM delta.`dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/`
// MAGIC WHERE details LIKE '%flow_progress%data_quality%expectations%'

// COMMAND ----------

import org.apache.spark.sql.functions.{lit, schema_of_json, from_json}
import collection.JavaConverters._

val df = sql("""SELECT id, origin, timestamp, details FROM delta.`dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/` WHERE details LIKE '%flow_progress%data_quality%expectations%'""")
val schema = schema_of_json(lit(df.select($"details").as[String].first))
val df_expectations = df.withColumn("details_json", from_json($"details", schema, Map[String, String]().asJava))

// COMMAND ----------

display(df_expectations)

// COMMAND ----------

df_expectations.createOrReplaceTempView("df_expectations")

// COMMAND ----------

// MAGIC %fs ls /user/denny.lee/nyctaxi/

// COMMAND ----------

// MAGIC %sql
// MAGIC DROP TABLE IF EXISTS DAIS21.expectations_log; 

// COMMAND ----------

dbutils.fs.rm("/user/denny.lee/nyctaxi/expectation_log", recurse=true)

// COMMAND ----------

// MAGIC %sql
// MAGIC -- SELECT id, origin.pipeline_id, origin.pipeline_name, origin.cluster_id, origin.flow_id, origin.flow_name, * FROM delta.`dbfs:/pipelines/59cf8076-61aa-488b-9139-edba476b0c91/system/events/`
// MAGIC -- WHERE details LIKE '%flow_progress%data_quality%expectations%'
// MAGIC CREATE TABLE DAIS21.expectations_log 
// MAGIC USING delta
// MAGIC LOCATION '/user/denny.lee/nyctaxi/expectation_log'
// MAGIC AS
// MAGIC SELECT id, timestamp, origin.pipeline_id, origin.pipeline_name, origin.cluster_id, origin.flow_id, origin.flow_name, details_json 
// MAGIC   FROM df_expectations

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT * FROM DAIS21.expectations_log ORDER BY timestamp DESC LIMIT 10

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT recent_run, time, flow_name, dropped_records, num_output_rows 
// MAGIC   FROM (
// MAGIC SELECT DATE_FORMAT(timestamp, 'HH:mm:ss') as time,
// MAGIC        cluster_id, 
// MAGIC        flow_name, 
// MAGIC        details_json.flow_progress.data_quality.dropped_records, 
// MAGIC        details_json.flow_progress.metrics.num_output_rows,
// MAGIC        dense_rank() OVER (PARTITION BY flow_name ORDER BY DATE_FORMAT(timestamp, 'HH:mm:ss') DESC) as recent_run
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  ORDER BY timestamp DESC 
// MAGIC  LIMIT 10
// MAGIC ) a

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT details_json 
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  WHERE flow_id = "c84482d6-32b4-4ce5-ba3a-0c9bd9c0248d"

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT details_json.flow_progress.data_quality 
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  WHERE flow_id = "c84482d6-32b4-4ce5-ba3a-0c9bd9c0248d"

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT details_json.flow_progress.data_quality.expectations[0].dataset AS dataset, 
// MAGIC        details_json.flow_progress.data_quality.expectations[0].failed_records AS failed_records,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].name AS expectation,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].passed_records AS passed_records
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  WHERE flow_id = "c84482d6-32b4-4ce5-ba3a-0c9bd9c0248d"

// COMMAND ----------

// MAGIC %sql
// MAGIC 
// MAGIC SELECT timestamp,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].dataset AS dataset, 
// MAGIC        details_json.flow_progress.data_quality.expectations[0].failed_records AS failed_records,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].name AS expectation,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].passed_records AS passed_records
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  ORDER BY timestamp DESC
// MAGIC  LIMIT 10
// MAGIC  

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT DATE_FORMAT(timestamp, 'HH:mm:ss') as time,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].dataset AS dataset, 
// MAGIC        details_json.flow_progress.data_quality.expectations[0].failed_records AS failed_records,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].name AS expectation,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].passed_records AS passed_records
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  ORDER BY timestamp DESC
// MAGIC  LIMIT 10
// MAGIC  

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT DISTINCT details_json.flow_progress.data_quality.expectations[0].dataset FROM DAIS21.expectations_log  

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT DATE_FORMAT(timestamp, 'HH:mm:ss') as time,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].dataset AS dataset, 
// MAGIC        details_json.flow_progress.data_quality.expectations[0].failed_records AS failed_records,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].name AS expectation,
// MAGIC        details_json.flow_progress.data_quality.expectations[0].passed_records AS passed_records
// MAGIC   FROM DAIS21.expectations_log 
// MAGIC  ORDER BY timestamp DESC
// MAGIC  LIMIT 10
// MAGIC  

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT * FROM DAIS21.bronze_GreenCab LIMIT 10

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT COUNT(1) FROM DAIS21.bronze_GreenCab WHERE total_amount < 3.00

// COMMAND ----------

// MAGIC %sql
// MAGIC SELECT COUNT(1) FROM DAIS21.bronze_GreenCab

// COMMAND ----------



